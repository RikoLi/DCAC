import math
import torch
import torch.nn as nn
from model.utlis import load_clip_to_cpu

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        print("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

def weights_init_classifier(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.normal_(m.weight, std=0.001)
        if m.bias:
            nn.init.constant_(m.bias, 0.0)
            
def weights_init_kaiming(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')
        nn.init.constant_(m.bias, 0.0)

    elif classname.find('Conv') != -1:
        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')
        if m.bias is not None:
            nn.init.constant_(m.bias, 0.0)
    elif classname.find('BatchNorm') != -1:
        if m.affine:
            nn.init.constant_(m.weight, 1.0)
            nn.init.constant_(m.bias, 0.0)

class CLIP_ImageEncoder(nn.Module):
    def __init__(self, cfg, num_classes, cam_num, view_num) -> None:
        super().__init__()
        self.cfg = cfg
        self.num_classes = num_classes
        self.num_cams = cam_num
        self.num_views = view_num
        self.h_resolution = int((cfg.INPUT.SIZE_TRAIN[0]-cfg.MODEL.STRIDE_SIZE[0])//cfg.MODEL.STRIDE_SIZE[0] + 1)
        self.w_resolution = int((cfg.INPUT.SIZE_TRAIN[1]-cfg.MODEL.STRIDE_SIZE[1])//cfg.MODEL.STRIDE_SIZE[1] + 1)
        self.num_patches = self.h_resolution * self.w_resolution
        
        clip_model = load_clip_to_cpu(cfg.MODEL.NAME, self.h_resolution, self.w_resolution, cfg.MODEL.STRIDE_SIZE[0]).cuda()
        
        self.image_encoder = clip_model.visual
        
        # classification head
        self.fc = nn.Linear(self.image_encoder.output_dim, num_classes, bias=False)
        self.fc.apply(weights_init_classifier)
        print(40*'-'+'\n', 'FC classifier check:')
        print(self.fc)
        print(40*'-')
        
        # bnneck
        self.bnneck = nn.BatchNorm1d(self.image_encoder.output_dim)
        self.bnneck.apply(self._init_bnneck)
        print(40*'-'+'\n', 'BN neck check:')
        print(self.bnneck)
        print(40*'-')
        
        self.enable_cam_emb = cfg.MODEL.ENABLE_CAM_EMB
        if self.enable_cam_emb:
            self.cam_emb = nn.Parameter(torch.empty(cam_num, 768))
            nn.init.trunc_normal_(self.cam_emb.data, std=0.02)
            print(f'Enable camera embedding with shape {self.cam_emb.shape}')
        
        
        # Trick: freeze patch projection for improved stability
        # https://arxiv.org/pdf/2104.02057.pdf
        print(40*'-'+'\n', 'PatchEmbed freeze check:')
        if cfg.MODEL.FREEZE_PATCH_PROJ:
            for _, v in self.image_encoder.conv1.named_parameters():
                v.requires_grad_(False)
            print('freeze patch projection layer with shape {}'.format(self.image_encoder.conv1.weight.shape))
        else:
            print('do not freeze patch projection layer')
        print(40*'-')
        
        # Efficient fine-tuning: freeze transformer blocks using given indices
        self.freeze_blocks(cfg.MODEL.FROZEN_BLOCKS)
        
    def load_clipreid_pretrained_image_encoder(self, path):
        param_dict = torch.load(path, map_location='cpu')
        cnt = 0
        for k, v in param_dict.items():
            if k.startswith('image_encoder.'): # ViT backbone
                self.state_dict()[k].copy_(v)
                cnt += 1
            elif k.startswith('classifier_proj.'): # classifier
                self.state_dict()[k.replace('classifier_proj', 'fc')].copy_(v)
                cnt += 1
            elif k.startswith('bottleneck_proj.'): # bnneck
                self.state_dict()[k.replace('bottleneck_proj', 'bnneck')].copy_(v)
                cnt += 1
                
        print(f'Load {cnt} layers from CLIP-ReID pretrained weight.')
    
    def freeze_blocks(self, block_list):
        if len(block_list) == 0:
            print('No blocks are frozen.')
        else:
            cnt = 0
            for i in block_list:
                for v in self.image_encoder.transformer.resblocks[i].parameters():
                    v.requires_grad_(False)
                    cnt += 1
            print(f'Freeze {cnt} params in transformer blocks {block_list}')
        
        
    def forward(self, img=None, cam_label=None, view_label=None):
        """
        img: [B, C, H, W]
        """
        if cam_label is not None:
            cv_emb = self.cam_emb[cam_label]
        else:
            cv_emb = None
        _, _, xproj = self.image_encoder(img, cv_emb) # [B, L, D]
        B = xproj.shape[0]
        xproj = xproj[:,0,:] # [B, D]
        
        bn_xproj = self.bnneck(xproj)
        logit = self.fc(bn_xproj)
        
        return logit, xproj

    @torch.no_grad()
    def infer_image(self, img, cam_label=None, view_label=None, after_bn=True):
        """
        img: [B, C, H, W]
        """
        _, _, xproj = self.image_encoder(img, cam_label) # [B, L, D]
        B = xproj.shape[0]
        xproj = xproj[:,0,:] # [B, D]
        
        bn_xproj = self.bnneck(xproj)
        
        if after_bn:
            return bn_xproj
        else:
            return xproj
        
    def load_param(self, trained_path):
        param_dict = torch.load(trained_path, map_location='cpu')
        for i in param_dict:
            self.state_dict()[i.replace('module.', '')].copy_(param_dict[i])
        print('Loading pretrained model from {}'.format(trained_path))
    
    def load_param_inference(self, trained_path):
        cnt = 0
        param_dict = torch.load(trained_path, map_location='cpu')
        for i in param_dict:
            if 'text_encoder' in i or 'prompt_learner' in i or 'fc' in i:
                if 'image_encoder' not in i:
                    cnt += 1
                    continue # ignore num_class related layers
            self.state_dict()[i.replace('module.', '')].copy_(param_dict[i])
        print(f'Loading pretrained model from {trained_path}, ignore {cnt} layers.')
    
    
    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        
    def _init_bnneck(self, m):
        if isinstance(m, nn.BatchNorm1d):
            nn.init.constant_(m.weight, 1.0)
            nn.init.constant_(m.bias, 0.0)
            m.bias.requires_grad_(False)
        
def make_single_grained_model(cfg, num_classes, cam_num, view_num):
    model = CLIP_ImageEncoder(cfg, num_classes, cam_num, view_num)
    return model